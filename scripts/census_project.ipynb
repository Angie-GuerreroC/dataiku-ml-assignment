{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422b2f66-2bf3-4477-9c6a-5e0738c7cbba",
   "metadata": {},
   "source": [
    "# Dataiku Census Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a7f05-5eae-4703-9113-8ffe33c3e67c",
   "metadata": {},
   "source": [
    "Goal: Identify the characteristics of individuals earning more or less than $50K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d95f74-b302-4431-b431-ee8634c5c51e",
   "metadata": {},
   "source": [
    "### Machine Learning Pipeline Script\n",
    "\n",
    "## Functionalities:\n",
    "1. **Data Loading and Preprocessing**:\n",
    "   - Handles missing values and duplicate records.\n",
    "   - Encodes categorical variables using label encoding.\n",
    "   - Scales numerical features for consistency.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "   - Visualizes data distributions and relationships (e.g., income vs. gender, education, race).\n",
    "   - Identifies outliers and trends in the dataset.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Creates new features (e.g., `capital_net_gain`).\n",
    "   - Selects the most relevant features using statistical methods (e.g., chi-square test).\n",
    "\n",
    "4. **Class Imbalance Handling**:\n",
    "   - Applies **SMOTE (Synthetic Minority Oversampling Technique)** to balance classes in the training dataset.\n",
    "\n",
    "5. **Model Training and Evaluation**:\n",
    "   - Implements and evaluates the following machine learning models:\n",
    "     - **Random Forest**\n",
    "     - **Logistic Regression**\n",
    "     - **XGBoost**\n",
    "   - Produces classification reports, confusion matrices, and precision-recall curves.\n",
    "\n",
    "6. **Model Comparison and Hyperparameter Tuning**:\n",
    "   - Compares model performance using **AUC (Area Under the Curve)** scores.\n",
    "   - Optimizes the XGBoost model using a randomized grid search.\n",
    "\n",
    "7. **Visualization**:\n",
    "   - Generates and saves:\n",
    "     - Feature importance plots.\n",
    "     - Model comparison plots.\n",
    "     - Validation and test performance metrics.\n",
    "\n",
    "## Dependencies:\n",
    "- `pandas`: For data manipulation and analysis.\n",
    "- `numpy`: For numerical computations.\n",
    "- `matplotlib`: For static visualizations.\n",
    "- `seaborn`: For enhanced data visualizations.\n",
    "- `scikit-learn`: For machine learning models and evaluation metrics.\n",
    "- `imbalanced-learn`: For handling class imbalance with SMOTE.\n",
    "- `xgboost`: For gradient boosting model implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf1e23-e9de-43d1-adf6-45cde0cbdb65",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5feda2d-4d3a-4455-9668-e4074ac8de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # For data splitting and hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier  # For Random Forest classification\n",
    "from sklearn.linear_model import LogisticRegression  # For Logistic Regression\n",
    "from xgboost import XGBClassifier  # For XGBoost classification\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder  # For scaling and encoding data\n",
    "from sklearn.metrics import (  # For model evaluation metrics\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, chi2  # For feature selection\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance through oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6334e-c317-42dd-92ab-840220d247ed",
   "metadata": {},
   "source": [
    "## Load & Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53481483-3394-47d1-9e2a-bdb201a1c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the training and test datasets.\n",
    "    \"\"\"\n",
    "    # Define column names for the datasets based on provided metadata\n",
    "    column_names = [\n",
    "        'age', 'class_of_worker', 'detailed_industry_recode', 'detailed_occupation_recode', 'education', \n",
    "        'wage_per_hour', 'enroll_in_edu_inst_last_wk', 'marital_stat', 'major_industry_code', \n",
    "        'major_occupation_code', 'race', 'hispanic_origin', 'sex', 'member_of_a_labor_union', 'reason_for_unemployment', \n",
    "        'full_or_part_time_employment_stat', 'capital_gains', 'capital_losses', 'dividends_from_stocks', \n",
    "        'tax_filer_stat', 'region_of_previous_residence', 'state_of_previous_residence', \n",
    "        'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'instance_weight', \n",
    "        'migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', \n",
    "        'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'num_persons_worked_for_employer', \n",
    "        'family_members_under_18', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', \n",
    "        'citizenship', 'own_business_or_self_employed', 'fill_inc_questionnaire_for_veterans_admin', \n",
    "        'veterans_benefits', 'weeks_worked_in_year', 'year', 'income'\n",
    "    ]\n",
    "    \n",
    "    # Load the training dataset\n",
    "    train_df = pd.read_csv(train_path, header=None)  # Load CSV without a header\n",
    "    train_df.columns = column_names  # Assign column names to the DataFrame\n",
    "    train_df.replace('?', np.nan, inplace=True)  # Replace \"?\" with NaN\n",
    "    train_df.dropna(inplace=True)  # Drop rows with missing values\n",
    "    train_df.drop_duplicates(inplace=True)  # Remove duplicate rows\n",
    "    if 'instance_weight' in train_df.columns:  # Drop unnecessary 'instance_weight' column if it exists\n",
    "        train_df.drop('instance_weight', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X = train_df.drop('income', axis=1)  # Features\n",
    "    y = train_df['income']  # Target variable\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y  # Stratified split to maintain class proportions\n",
    "    )\n",
    "\n",
    "    # Load the test dataset\n",
    "    test_df = pd.read_csv(test_path, header=None)  # Load CSV without a header\n",
    "    test_df.columns = column_names  # Assign column names\n",
    "    test_df.replace('?', np.nan, inplace=True)  # Replace \"?\" with NaN\n",
    "    test_df.dropna(inplace=True)  # Drop rows with missing values\n",
    "    test_df.drop_duplicates(inplace=True)  # Remove duplicate rows\n",
    "    if 'instance_weight' in test_df.columns:  # Drop unnecessary 'instance_weight' column if it exists\n",
    "        test_df.drop('instance_weight', axis=1, inplace=True)\n",
    "\n",
    "    # Return the cleaned training dataset, splits for training and validation, and the cleaned test dataset\n",
    "    return train_df, (X_train_split, X_val_split, y_train_split, y_val_split), test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe765e3-4b41-4857-aa95-319f2dd3ad47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Notes\n",
    "\n",
    "- **Initial Observations**:\n",
    "  - The dataset exhibits a high class imbalance, with the majority of instances labeled as `-50,000`.\n",
    "  - A significant number of duplicate entries exist in both the learning and testing datasets.\n",
    "    \n",
    "\n",
    "- **Assumptions and Prior Knowledge**:\n",
    "  - Higher educational attainment is generally associated with higher salaries.\n",
    "  - Individuals in their 30s to 40s are more likely to have higher incomes (peak income years).\n",
    "  - There are racial disparities in income levels (a bias could be present in model).\n",
    "  - There are gender disparities in income levels (a bias could be present in model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67f628-b55f-4137-8763-eff0a9b7f911",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c1a539-d3de-44dc-b8b0-adf68491e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, splits, test_df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by performing feature engineering, label encoding, \n",
    "    and target variable transformation.\n",
    "    \"\"\"\n",
    "    # Unpack splits\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = splits\n",
    "\n",
    "    # Feature engineering: Calculate net capital gain and drop redundant columns\n",
    "    for dataset in [train_df, X_train_split, X_val_split]:\n",
    "        dataset['capital_net_gain'] = dataset['capital_gains'] - dataset['capital_losses']  # Calculate net capital gain\n",
    "        dataset.drop(columns=['capital_gains', 'capital_losses'], inplace=True)  # Drop original columns\n",
    "\n",
    "    # Apply the same transformation to the test dataset\n",
    "    test_df['capital_net_gain'] = test_df['capital_gains'] - test_df['capital_losses']  # Calculate net capital gain\n",
    "    test_df.drop(columns=['capital_gains', 'capital_losses'], inplace=True)  # Drop original columns\n",
    "\n",
    "    # Initialize a dictionary to store LabelEncoders for each categorical column\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Encode categorical features using LabelEncoder\n",
    "    for column in train_df.select_dtypes(include=['object']).columns:\n",
    "        if column != 'income':  # Skip the target variable\n",
    "            le = LabelEncoder()  # Initialize LabelEncoder\n",
    "            train_df[column] = le.fit_transform(train_df[column])  # Fit and transform training data\n",
    "            X_train_split[column] = le.transform(X_train_split[column])  # Transform training split\n",
    "            X_val_split[column] = le.transform(X_val_split[column])  # Transform validation split\n",
    "            test_df[column] = le.transform(test_df[column])  # Transform test data\n",
    "            label_encoders[column] = le  # Store the encoder\n",
    "\n",
    "    # Convert the target variable (`income`) to binary\n",
    "    train_df['income'] = train_df['income'].apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "    test_df['income'] = test_df['income'].apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "    y_train_split = y_train_split.apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "    y_val_split = y_val_split.apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "\n",
    "    # Return the preprocessed datasets and the label encoders\n",
    "    return train_df, (X_train_split, X_val_split, y_train_split, y_val_split), test_df, label_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcbc7c-611f-4b9a-a45d-f6646892a167",
   "metadata": {},
   "source": [
    "## Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d145de-071a-4d2d-b821-132ff2c22377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(train_df):\n",
    "    \"\"\"\n",
    "    Perform Exploratory Data Analysis (EDA) on the training dataset.\n",
    "    This function generates various visualizations and saves them as image files \n",
    "    to analyze the distribution and relationships of key features in the dataset.\n",
    "    \"\"\"\n",
    "    # Basic statistics summary\n",
    "    print(\"Basic Statistics of Train Data:\")\n",
    "    print(train_df.describe())\n",
    "\n",
    "    # Plot: Distribution of Income\n",
    "    print(\"\\nDistribution of Income:\")\n",
    "    sns.countplot(data=train_df, x='income', palette=\"viridis\")\n",
    "    plt.title('Income Distribution')\n",
    "    plt.xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()  # Ensure labels are not cut off\n",
    "    plt.savefig('eda_income_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Income vs Gender\n",
    "    print(\"\\nIncome vs Gender:\")\n",
    "    sns.countplot(data=train_df, x='sex', hue='income', palette=\"viridis\")\n",
    "    plt.title('Income by Gender')\n",
    "    plt.xlabel('Gender (0 = Female, 1 = Male)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_income_by_gender.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Income vs Education\n",
    "    print(\"\\nIncome vs Education:\")\n",
    "    education_mapping = {\n",
    "        0: \"Children\", 1: \"7th and 8th grade\", 2: \"9th grade\", 3: \"10th grade\",\n",
    "        4: \"High school graduate\", 5: \"11th grade\", 6: \"12th grade no diploma\",\n",
    "        7: \"5th or 6th grade\", 8: \"Less than 1st grade\", 9: \"Bachelors degree\",\n",
    "        10: \"1st to 4th grade\", 11: \"Some college but no degree\",\n",
    "        12: \"Masters degree\", 13: \"Associates degree-occup/vocational\",\n",
    "        14: \"Associates degree-academic program\", 15: \"Doctorate degree\",\n",
    "        16: \"Prof school degree\"\n",
    "    }\n",
    "    train_df['education_label'] = train_df['education'].map(education_mapping)\n",
    "    sns.countplot(data=train_df, y='education_label', hue='income', palette=\"viridis\",\n",
    "                  order=train_df['education_label'].value_counts().index)\n",
    "    plt.title('Income by Education Level')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Education Level')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_income_by_education.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Income vs Race\n",
    "    print(\"\\nIncome vs Race:\")\n",
    "    race_mapping = {\n",
    "        0: \"White\", 1: \"Black\", 2: \"Asian or Pacific Islander\",\n",
    "        3: \"Amer Indian Aleut or Eskimo\", 4: \"Other\"\n",
    "    }\n",
    "    train_df['race_label'] = train_df['race'].map(race_mapping)\n",
    "    sns.countplot(data=train_df, y='race_label', hue='income', palette=\"viridis\",\n",
    "                  order=train_df['race_label'].value_counts().index)\n",
    "    plt.title('Income by Race')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Race')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_income_by_race.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Age Distribution vs Income\n",
    "    print(\"\\nAge Distribution vs Income:\")\n",
    "    sns.histplot(data=train_df, x='age', hue='income', bins=20, kde=True, palette=\"viridis\", multiple=\"stack\")\n",
    "    plt.title('Age Distribution by Income')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_age_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Boxplots for Wage, Weeks Worked, and Capital Net Gain\n",
    "    print(\"\\nBoxplots for Wage, Weeks Worked, and Capital Net Gain:\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    sns.boxplot(data=train_df, x='income', y='wage_per_hour', ax=axes[0], hue='income', palette=\"viridis\")\n",
    "    axes[0].set_title('Wage per Hour by Income')\n",
    "    axes[0].set_xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    axes[0].set_ylabel('Wage per Hour')\n",
    "    axes[0].legend_.remove()\n",
    "\n",
    "    sns.boxplot(data=train_df, x='income', y='weeks_worked_in_year', ax=axes[1], hue='income', palette=\"viridis\")\n",
    "    axes[1].set_title('Weeks Worked by Income')\n",
    "    axes[1].set_xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    axes[1].set_ylabel('Weeks Worked')\n",
    "    axes[1].legend_.remove()\n",
    "\n",
    "    sns.boxplot(data=train_df, x='income', y='capital_net_gain', ax=axes[2], hue='income', palette=\"viridis\")\n",
    "    axes[2].set_title('Capital Net Gain by Income')\n",
    "    axes[2].set_xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    axes[2].set_ylabel('Capital Net Gain')\n",
    "    axes[2].legend_.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_boxplots.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Remove the temporary education label column\n",
    "    train_df.drop(columns=['education_label'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38657b-0b11-4653-9c26-b6a5d0420041",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Notes\n",
    "\n",
    "- **Observations**:\n",
    "  - Men are more likely to have higher incomes.\n",
    "  - Bachelor's, Associate's, and Master's degree holders tend to have higher incomes.\n",
    "  - Outliers observed in `wage_per_hour`.\n",
    "  - Outliers observed in `capital_net_gain`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e943526-eed4-486e-a6ec-e88c0c6823cb",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1486d0d8-a37c-4b3e-be9a-9b292ccfb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(splits, test_df, k=10):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on training, validation, and test datasets.\n",
    "    This function scales the data, selects the top K features using the Chi-square test, \n",
    "    and retains only the selected features for model training and evaluation.\n",
    "    \"\"\"\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = splits\n",
    "\n",
    "    # Scale data (fit scaler only on training data)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_split_scaled = pd.DataFrame(scaler.fit_transform(X_train_split), columns=X_train_split.columns)\n",
    "    X_val_split_scaled = pd.DataFrame(scaler.transform(X_val_split), columns=X_val_split.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(test_df.drop(columns=['income'])), columns=test_df.drop(columns=['income']).columns)\n",
    "\n",
    "    # Select K best features (fit selector only on training data)\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "    X_train_split_selected = selector.fit_transform(X_train_split_scaled, y_train_split)\n",
    "    X_val_split_selected = selector.transform(X_val_split_scaled)\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "    # Retain only selected feature names\n",
    "    selected_features = X_train_split.columns[selector.get_support()]\n",
    "    X_train_split_selected = pd.DataFrame(X_train_split_selected, columns=selected_features)\n",
    "    X_val_split_selected = pd.DataFrame(X_val_split_selected, columns=selected_features)\n",
    "    X_test_selected = pd.DataFrame(X_test_selected, columns=selected_features)\n",
    "\n",
    "    return (X_train_split_selected, X_val_split_selected, y_train_split, y_val_split), X_test_selected, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3201e-d8e1-47b4-afc9-454141f213b6",
   "metadata": {},
   "source": [
    "## Imbalance & Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41560804-ae84-4dd2-8bbc-b74889ef3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X_train_split, y_train_split):\n",
    "    \"\"\"\n",
    "    Balances the training data using SMOTE (Synthetic Minority Oversampling Technique) to handle class imbalance.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train_split, y_train_split)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def train_and_evaluate_models(X_train_split, X_val_split, y_train_split, y_val_split, selected_features):\n",
    "    \"\"\"\n",
    "    Trains and evaluates three machine learning models (Random Forest, Logistic Regression, XGBoost) on the training and validation datasets. Outputs performance metrics and feature importance plots.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    rf_model = RandomForestClassifier(random_state=42, n_estimators=100, class_weight=\"balanced\")\n",
    "    print(\"Random Forest:\")\n",
    "    rf_auc = evaluate_on_validation(rf_model, X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "    plot_feature_importance(rf_model, selected_features, model_name=\"Random Forest\")\n",
    "    results['Random Forest'] = {'AUC': rf_auc}\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
    "    print(\"Logistic Regression:\")\n",
    "    lr_auc = evaluate_on_validation(lr_model, X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "    results['Logistic Regression'] = {'AUC': lr_auc}\n",
    "\n",
    "    scale_pos_weight = (len(y_train_split) - sum(y_train_split)) / sum(y_train_split)\n",
    "    print(f\"Calculated scale_pos_weight for XGBoost: {scale_pos_weight:.2f}\")\n",
    "\n",
    "    xgb_model = XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight, eval_metric='logloss')\n",
    "    print(\"XGBoost:\")\n",
    "    xgb_auc = evaluate_on_validation(xgb_model, X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "    plot_feature_importance(xgb_model, selected_features, model_name=\"XGBoost\")\n",
    "    results['XGBoost'] = {'AUC': xgb_auc}\n",
    "\n",
    "    return results\n",
    "\n",
    "def hyperparameter_tuning(X_train_split, y_train_split, X_val_split, y_val_split):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning on the XGBoost model using RandomizedSearchCV to optimize its parameters for better performance.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb_random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "    best_params = xgb_random_search.best_params_\n",
    "    print(\"Best Parameters for XGBoost:\", best_params)\n",
    "\n",
    "    xgb_optimized = XGBClassifier(**best_params, random_state=42, eval_metric='logloss')\n",
    "    xgb_optimized.fit(X_train_split, y_train_split)\n",
    "\n",
    "    return xgb_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaa88f-85df-4f38-abc3-fad4930ec5e8",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a07162-1926-4bf1-90d6-eb241ece8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_validation(model, X_train_split, X_val_split, y_train_split, y_val_split):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained model on the validation set.\n",
    "    Outputs a classification report and AUC score.\n",
    "    \"\"\"\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    predictions = model.predict(X_val_split)\n",
    "    probabilities = model.predict_proba(X_val_split)[:, 1]\n",
    "\n",
    "    print(\"\\nValidation Set Evaluation:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val_split, predictions))\n",
    "    print(f\"AUC Score: {roc_auc_score(y_val_split, probabilities):.4f}\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_val_split, probabilities)\n",
    "    plt.plot(recall, precision, label=f'{type(model).__name__}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (Validation)')\n",
    "    plt.legend()\n",
    "    filename_prc = f\"precision_recall_curve_validation_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_prc)\n",
    "    print(f\"Precision-Recall Curve saved as {filename_prc}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_val_split, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix (Validation)\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    filename_cm = f\"confusion_matrix_validation_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_cm)\n",
    "    print(f\"Confusion Matrix saved as {filename_cm}\")\n",
    "    plt.close()\n",
    "\n",
    "    return roc_auc_score(y_val_split, probabilities)\n",
    "\n",
    "def evaluate_on_test(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the final model's performance on the test set.\n",
    "    Outputs a classification report and AUC score.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(f\"AUC Score: {roc_auc_score(y_test, probabilities):.4f}\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probabilities)\n",
    "    plt.plot(recall, precision, label=f'{type(model).__name__}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (Test)')\n",
    "    plt.legend()\n",
    "    filename_prc = f\"precision_recall_curve_test_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_prc)\n",
    "    print(f\"Precision-Recall Curve saved as {filename_prc}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix (Test)\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    filename_cm = f\"confusion_matrix_test_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_cm)\n",
    "    print(f\"Confusion Matrix saved as {filename_cm}\")\n",
    "    plt.close()\n",
    "\n",
    "    return roc_auc_score(y_test, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6ab7f-fec8-42de-9b31-aaead14b23bf",
   "metadata": {},
   "source": [
    "## Top Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aefaffa8-53d7-4bd3-a0ce-f0c182043821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, features, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Visualizes the importance of features for models that support the feature_importances_ attribute (e.g., Random Forest, XGBoost).\n",
    "    Saves the feature importance plot to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The machine learning model.\n",
    "        features (list): A list of feature names.\n",
    "        model_name (str): The name of the model (used for saving the plot).\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importances, x='Importance', y='Feature', dodge=False)\n",
    "        plt.title(f\"Feature Importance - {model_name}\")\n",
    "        plt.xlabel(\"Feature Importance Score\")\n",
    "        plt.ylabel(\"Features\")\n",
    "        \n",
    "        # Adjust layout to prevent labels from getting cut off\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        filename = f\"feature_importance_{model_name.replace(' ', '_').lower()}.png\"\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Feature importance plot saved as {filename}\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253acfa-916c-4662-ba95-19b02d547c8c",
   "metadata": {},
   "source": [
    "## Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79afd0e0-2bb4-4a8a-b131-60a34fa13855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(results):\n",
    "    \"\"\"\n",
    "    Compares the performance of machine learning models based on their AUC scores.\n",
    "    Saves the comparison plot to a file.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing model names as keys and a dictionary of metrics (e.g., AUC) as values.\n",
    "    \"\"\"\n",
    "    print(\"\\n### Model Comparison Recap ###\")\n",
    "    model_names = list(results.keys())\n",
    "    metrics = ['AUC']\n",
    "\n",
    "    data = []\n",
    "    for model, metrics_dict in results.items():\n",
    "        row = [model] + [metrics_dict.get(metric, \"N/A\") for metric in metrics]\n",
    "        data.append(row)\n",
    "\n",
    "    comparison_df = pd.DataFrame(data, columns=['Model'] + metrics)\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Plot Model AUC Comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"AUC\", y=\"Model\", data=comparison_df, palette=\"viridis\")\n",
    "    plt.title(\"Model AUC Comparison\")\n",
    "    plt.xlabel(\"AUC Score\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    \n",
    "    # Adjust layout to avoid labels getting cut off\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    filename = \"model_auc_comparison.png\"\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    print(f\"Model AUC Comparison saved as {filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdd99a-c26a-48de-a436-30bd0a43b9cd",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c96f41-ef9b-48f2-a1af-5fc2f68633ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Statistics of Train Data:\n",
      "                 age  class_of_worker  detailed_industry_recode  \\\n",
      "count  196294.000000    196294.000000             196294.000000   \n",
      "mean       34.929468         3.493286                 15.603187   \n",
      "std        22.210001         1.112499                 18.106401   \n",
      "min         0.000000         0.000000                  0.000000   \n",
      "25%        16.000000         3.000000                  0.000000   \n",
      "50%        34.000000         3.000000                  1.000000   \n",
      "75%        50.000000         4.000000                 33.000000   \n",
      "max        90.000000         8.000000                 51.000000   \n",
      "\n",
      "       detailed_occupation_recode      education  wage_per_hour  \\\n",
      "count               196294.000000  196294.000000  196294.000000   \n",
      "mean                    11.490468      10.039339      56.336505   \n",
      "std                     14.498128       4.151615     277.054333   \n",
      "min                      0.000000       0.000000       0.000000   \n",
      "25%                      0.000000       9.000000       0.000000   \n",
      "50%                      2.000000      10.000000       0.000000   \n",
      "75%                     26.000000      12.000000       0.000000   \n",
      "max                     46.000000      16.000000    9999.000000   \n",
      "\n",
      "       enroll_in_edu_inst_last_wk   marital_stat  major_industry_code  \\\n",
      "count               196294.000000  196294.000000        196294.000000   \n",
      "mean                     1.907226       2.989490            12.989872   \n",
      "std                      0.376869       1.413935             4.815376   \n",
      "min                      0.000000       0.000000             0.000000   \n",
      "25%                      2.000000       2.000000            11.000000   \n",
      "50%                      2.000000       3.000000            14.000000   \n",
      "75%                      2.000000       4.000000            14.000000   \n",
      "max                      2.000000       6.000000            23.000000   \n",
      "\n",
      "       major_occupation_code  ...  country_of_birth_mother  \\\n",
      "count          196294.000000  ...            196294.000000   \n",
      "mean                6.307060  ...                35.605913   \n",
      "std                 3.129627  ...                10.440558   \n",
      "min                 0.000000  ...                 0.000000   \n",
      "25%                 6.000000  ...                40.000000   \n",
      "50%                 6.000000  ...                40.000000   \n",
      "75%                 7.000000  ...                40.000000   \n",
      "max                14.000000  ...                42.000000   \n",
      "\n",
      "       country_of_birth_self    citizenship  own_business_or_self_employed  \\\n",
      "count          196294.000000  196294.000000                  196294.000000   \n",
      "mean               37.467691       3.612225                       0.178304   \n",
      "std                 8.235906       1.119098                       0.557739   \n",
      "min                 0.000000       0.000000                       0.000000   \n",
      "25%                40.000000       4.000000                       0.000000   \n",
      "50%                40.000000       4.000000                       0.000000   \n",
      "75%                40.000000       4.000000                       0.000000   \n",
      "max                42.000000       4.000000                       2.000000   \n",
      "\n",
      "       fill_inc_questionnaire_for_veterans_admin  veterans_benefits  \\\n",
      "count                              196294.000000      196294.000000   \n",
      "mean                                    0.993877           1.538183   \n",
      "std                                     0.100349           0.836813   \n",
      "min                                     0.000000           0.000000   \n",
      "25%                                     1.000000           2.000000   \n",
      "50%                                     1.000000           2.000000   \n",
      "75%                                     1.000000           2.000000   \n",
      "max                                     2.000000           2.000000   \n",
      "\n",
      "       weeks_worked_in_year           year         income  capital_net_gain  \n",
      "count         196294.000000  196294.000000  196294.000000     196294.000000  \n",
      "mean              23.553889      94.499328       0.063079        403.942443  \n",
      "std               24.428588       0.500001       0.243105       4747.133419  \n",
      "min                0.000000      94.000000       0.000000      -4608.000000  \n",
      "25%                0.000000      94.000000       0.000000          0.000000  \n",
      "50%               12.000000      94.000000       0.000000          0.000000  \n",
      "75%               52.000000      95.000000       0.000000          0.000000  \n",
      "max               52.000000      95.000000       1.000000      99999.000000  \n",
      "\n",
      "[8 rows x 40 columns]\n",
      "\n",
      "Distribution of Income:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/_3kz3q7542g8wy3grh7rl0kc0000gp/T/ipykernel_29580/1124886381.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(data=train_df, x='income', palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Income vs Gender:\n",
      "\n",
      "Income vs Education:\n",
      "\n",
      "Income vs Race:\n",
      "\n",
      "Age Distribution vs Income:\n",
      "\n",
      "Boxplots for Wage, Weeks Worked, and Capital Net Gain:\n",
      "Random Forest:\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95     36783\n",
      "           1       0.34      0.50      0.41      2476\n",
      "\n",
      "    accuracy                           0.91     39259\n",
      "   macro avg       0.66      0.72      0.68     39259\n",
      "weighted avg       0.93      0.91      0.92     39259\n",
      "\n",
      "AUC Score: 0.8762\n",
      "Precision-Recall Curve saved as precision_recall_curve_validation_randomforestclassifier.png\n",
      "Confusion Matrix saved as confusion_matrix_validation_randomforestclassifier.png\n",
      "Feature importance plot saved as feature_importance_random_forest.png\n",
      "Logistic Regression:\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.75      0.85     36783\n",
      "           1       0.19      0.88      0.31      2476\n",
      "\n",
      "    accuracy                           0.75     39259\n",
      "   macro avg       0.59      0.81      0.58     39259\n",
      "weighted avg       0.94      0.75      0.82     39259\n",
      "\n",
      "AUC Score: 0.8907\n",
      "Precision-Recall Curve saved as precision_recall_curve_validation_logisticregression.png\n",
      "Confusion Matrix saved as confusion_matrix_validation_logisticregression.png\n",
      "Calculated scale_pos_weight for XGBoost: 1.00\n",
      "XGBoost:\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94     36783\n",
      "           1       0.32      0.67      0.43      2476\n",
      "\n",
      "    accuracy                           0.89     39259\n",
      "   macro avg       0.65      0.79      0.69     39259\n",
      "weighted avg       0.93      0.89      0.91     39259\n",
      "\n",
      "AUC Score: 0.9006\n",
      "Precision-Recall Curve saved as precision_recall_curve_validation_xgbclassifier.png\n",
      "Confusion Matrix saved as confusion_matrix_validation_xgbclassifier.png\n",
      "Feature importance plot saved as feature_importance_xgboost.png\n",
      "\n",
      "### Model Comparison Recap ###\n",
      "                 Model       AUC\n",
      "0        Random Forest  0.876175\n",
      "1  Logistic Regression  0.890704\n",
      "2              XGBoost  0.900613\n",
      "Model AUC Comparison saved as model_auc_comparison.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/_3kz3q7542g8wy3grh7rl0kc0000gp/T/ipykernel_29580/597264213.py:23: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"AUC\", y=\"Model\", data=comparison_df, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "\n",
      "Test Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95     92693\n",
      "           1       0.34      0.62      0.44      6186\n",
      "\n",
      "    accuracy                           0.90     98879\n",
      "   macro avg       0.66      0.77      0.69     98879\n",
      "weighted avg       0.93      0.90      0.91     98879\n",
      "\n",
      "AUC Score: 0.9032\n",
      "Precision-Recall Curve saved as precision_recall_curve_test_xgbclassifier.png\n",
      "Confusion Matrix saved as confusion_matrix_test_xgbclassifier.png\n",
      "Final Test AUC: 0.9032\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_path = '/Users/angieguerrero/Desktop/Dataiku Project/census_income_learn.csv'\n",
    "    test_path = '/Users/angieguerrero/Desktop/Dataiku Project/census_income_test.csv'\n",
    "\n",
    "    # Load and clean data\n",
    "    train_df, splits, test_df = load_and_clean_data(train_path, test_path)\n",
    "\n",
    "    # Preprocessing\n",
    "    train_df, splits, test_df, encoders = preprocess_data(train_df, splits, test_df)\n",
    "\n",
    "    # Perform initial EDA\n",
    "    perform_eda(train_df)\n",
    "\n",
    "    # Feature engineering\n",
    "    k_features = 10\n",
    "    splits, X_test_selected, selected_features = feature_engineering(splits, test_df, k=k_features)\n",
    "\n",
    "    # Handle class imbalance on training data\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = splits\n",
    "    X_train_split, y_train_split = handle_imbalance(X_train_split, y_train_split)\n",
    "\n",
    "    # Train and evaluate models on validation set\n",
    "    results = train_and_evaluate_models(X_train_split, X_val_split, y_train_split, y_val_split, selected_features)\n",
    "\n",
    "    # Compare models\n",
    "    model_comparison(results)\n",
    "\n",
    "    # Hyperparameter tuning for the best model (XGBoost in this case)\n",
    "    xgb_optimized = hyperparameter_tuning(X_train_split, y_train_split, X_val_split, y_val_split)\n",
    "\n",
    "    # Evaluate the optimized model on the test set\n",
    "    y_test = test_df['income']\n",
    "    test_auc = evaluate_on_test(xgb_optimized, X_test_selected, y_test)\n",
    "    print(f\"Final Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc793d-b8fa-4c62-925a-7d19ff9b935f",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "#### **Basic Statistics of Train Data**\n",
    "- **Demographics**:\n",
    "  - **Mean Age**: 34.93 years\n",
    "  - **Education Mean**: 10.04 (Indicating at least a high school education level on average)\n",
    "- **Economic Indicators**:\n",
    "  - **Wage per Hour**: Mean = $56.34, Max = $9,999 (Significant outliers identified)\n",
    "  - **Weeks Worked in Year**: Mean = 23.55 weeks, Max = 52 weeks (Indicating part-time employment for most individuals)\n",
    "  - **Capital Net Gain**: Mean = $403.94, Max = $99,999 (Substantial outliers detected)\n",
    "- **Income Distribution**:\n",
    "  - **Low Income (<$50K)**: 93.69% of individuals\n",
    "  - **High Income (≥$50K)**: Only 6.31% of individuals, highlighting class imbalance.\n",
    "\n",
    "#### **Key Insights from Exploratory Data Analysis**\n",
    "- **Income Distribution**:\n",
    "  - A stark imbalance exists between low-income and high-income groups.\n",
    "- **Income by Gender**:\n",
    "  - Men are significantly more likely to belong to the high-income group than women.\n",
    "- **Income by Education**:\n",
    "  - Advanced degrees (Bachelor's, Associate's, and Master's) are highly correlated with higher incomes.\n",
    "- **Income by Race**:\n",
    "  - White individuals form the largest group of high-income earners, whereas minority races have a much lower representation in this category.\n",
    "- **Age Distribution and Income**:\n",
    "  - Individuals aged 30–50 are most likely to earn high incomes.\n",
    "- **Outliers**:\n",
    "  - Extreme values are evident in `wage_per_hour` and `capital_net_gain`, requiring careful consideration during modeling.\n",
    "\n",
    "#### **Model Performance on Validation Set**\n",
    "1. **Random Forest**:\n",
    "   - **AUC**: 0.876\n",
    "   - **F1-Score (High Income)**: 0.41\n",
    "   - **Precision (High Income)**: 34%\n",
    "   - **Recall (High Income)**: 50%\n",
    "2. **Logistic Regression**:\n",
    "   - **AUC**: 0.891\n",
    "   - **F1-Score (High Income)**: 0.31\n",
    "   - **Precision (High Income)**: 19%\n",
    "   - **Recall (High Income)**: 88%\n",
    "3. **XGBoost**:\n",
    "   - **AUC**: 0.901\n",
    "   - **F1-Score (High Income)**: 0.43\n",
    "   - **Precision (High Income)**: 32%\n",
    "   - **Recall (High Income)**: 67%\n",
    "\n",
    "#### **Final Test Evaluation (XGBoost)**\n",
    "- **AUC**: 0.903 (Highest performance among all models)\n",
    "- **Precision (High Income)**: 34% (Moderate accuracy in identifying high-income individuals)\n",
    "- **Recall (High Income)**: 62% (Captures a significant portion of the high-income group)\n",
    "- **Accuracy**: 90% (High overall classification accuracy)\n",
    "\n",
    "#### **Conclusion**\n",
    "- **XGBoost** is the best-performing model, demonstrating the highest AUC (0.903) and an effective balance between precision and recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
