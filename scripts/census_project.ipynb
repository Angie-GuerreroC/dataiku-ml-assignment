{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85ca029-2759-444b-bcd6-c36d6593e7e0",
   "metadata": {},
   "source": [
    "## Dataiku ML Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a7f05-5eae-4703-9113-8ffe33c3e67c",
   "metadata": {},
   "source": [
    "Goal: Identify the characteristics of individuals earning more or less than $50K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf1e23-e9de-43d1-adf6-45cde0cbdb65",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5feda2d-4d3a-4455-9668-e4074ac8de0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np  # For numerical computations\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import seaborn as sns  # For statistical data visualization\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # For data splitting and hyperparameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier  # For Random Forest classification\n",
    "from sklearn.linear_model import LogisticRegression  # For Logistic Regression\n",
    "from xgboost import XGBClassifier  # For XGBoost classification\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder  # For scaling and encoding data\n",
    "from sklearn.metrics import (  # For model evaluation metrics\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, chi2  # For feature selection\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance through oversampling\n",
    "from sklearn.metrics import f1_score # F-1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6334e-c317-42dd-92ab-840220d247ed",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53481483-3394-47d1-9e2a-bdb201a1c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the training and test datasets.\n",
    "    \"\"\"\n",
    "    # Define column names for the datasets based on provided metadata\n",
    "    column_names = [\n",
    "        'age', 'class_of_worker', 'detailed_industry_recode', 'detailed_occupation_recode', 'education', \n",
    "        'wage_per_hour', 'enroll_in_edu_inst_last_wk', 'marital_stat', 'major_industry_code', \n",
    "        'major_occupation_code', 'race', 'hispanic_origin', 'sex', 'member_of_a_labor_union', 'reason_for_unemployment', \n",
    "        'full_or_part_time_employment_stat', 'capital_gains', 'capital_losses', 'dividends_from_stocks', \n",
    "        'tax_filer_stat', 'region_of_previous_residence', 'state_of_previous_residence', \n",
    "        'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'instance_weight', \n",
    "        'migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', \n",
    "        'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'num_persons_worked_for_employer', \n",
    "        'family_members_under_18', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', \n",
    "        'citizenship', 'own_business_or_self_employed', 'fill_inc_questionnaire_for_veterans_admin', \n",
    "        'veterans_benefits', 'weeks_worked_in_year', 'year', 'income'\n",
    "    ]\n",
    "    \n",
    "    # Load the training dataset\n",
    "    train_df = pd.read_csv(train_path, header=None)  # Load CSV without a header\n",
    "    train_df.columns = column_names  # Assign column names to the DataFrame\n",
    "    \n",
    "    # Load the test dataset\n",
    "    test_df = pd.read_csv(test_path, header=None)  # Load CSV without a header\n",
    "    test_df.columns = column_names  # Assign column names\n",
    "\n",
    "    # Return the raw loaded datasets for preprocessing\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a583da5-a552-4277-9fc9-bc0f039c1554",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf14904-1abc-4810-a795-0655394ccc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Handle missing values, filter rows, and drop duplicates.\n",
    "    Returns cleaned and split training data, along with cleaned training and test datasets.\n",
    "    Includes checks for income distribution before and after cleaning.\n",
    "    \"\"\"\n",
    "    # Store original row counts\n",
    "    original_train_rows = train_df.shape[0]\n",
    "    original_test_rows = test_df.shape[0]\n",
    "\n",
    "    # Log income distribution before cleaning\n",
    "    print(\"\\nIncome distribution (training data) before cleaning:\")\n",
    "    print(train_df['income'].value_counts())\n",
    "    print(\"\\nIncome distribution (test data) before cleaning:\")\n",
    "    print(test_df['income'].value_counts())\n",
    "\n",
    "    # Clean training data\n",
    "    train_df.replace(['?', '', ' '], np.nan, inplace=True)  # Replace placeholders with NaN\n",
    "    train_df.dropna(inplace=True)  # Drop rows with missing values\n",
    "    train_df.drop_duplicates(inplace=True)  # Drop duplicate rows\n",
    "    train_df = train_df[\n",
    "        (train_df['age'] >= 18)].copy()  # Explicit copy to avoid warnings\n",
    "    if 'instance_weight' in train_df.columns:  # Drop 'instance_weight' column if it exists\n",
    "        train_df.drop('instance_weight', axis=1, inplace=True)\n",
    "\n",
    "    # Clean test data\n",
    "    test_df.replace(['?', '', ' '], np.nan, inplace=True)  # Replace placeholders with NaN\n",
    "    test_df.dropna(inplace=True)  # Drop rows with missing values\n",
    "    test_df.drop_duplicates(inplace=True)  # Drop duplicate rows\n",
    "    test_df = test_df[\n",
    "        (test_df['age'] >= 18)].copy()  # Explicit copy to avoid warnings\n",
    "    if 'instance_weight' in test_df.columns:  # Drop 'instance_weight' column if it exists\n",
    "        test_df.drop('instance_weight', axis=1, inplace=True)\n",
    "\n",
    "    # Count rows after cleaning\n",
    "    cleaned_train_rows = train_df.shape[0]\n",
    "    cleaned_test_rows = test_df.shape[0]\n",
    "\n",
    "    # Log the number of rows before and after cleaning\n",
    "    print(f\"\\nTraining Data: {original_train_rows} rows before cleaning, {cleaned_train_rows} rows after cleaning.\")\n",
    "    print(f\"Test Data: {original_test_rows} rows before cleaning, {cleaned_test_rows} rows after cleaning.\")\n",
    "\n",
    "    # Log income distribution after cleaning\n",
    "    print(\"\\nIncome distribution (training data) after cleaning:\")\n",
    "    print(train_df['income'].value_counts())\n",
    "    print(\"\\nIncome distribution (test data) after cleaning:\")\n",
    "    print(test_df['income'].value_counts())\n",
    "\n",
    "    # Split the training data into features and target variable\n",
    "    X_train = train_df.drop('income', axis=1)  # Features\n",
    "    y_train = train_df['income']  # Target variable\n",
    "\n",
    "    # Split training data into training and validation sets\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    # Return splits, cleaned test data, and cleaned training data\n",
    "    return train_df, (X_train_split, X_val_split, y_train_split, y_val_split), test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcbc7c-611f-4b9a-a45d-f6646892a167",
   "metadata": {},
   "source": [
    "## Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d145de-071a-4d2d-b821-132ff2c22377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(train_df):\n",
    "    \"\"\"\n",
    "    Perform Exploratory Data Analysis (EDA) on the training dataset.\n",
    "    This function generates various visualizations and saves them as image files \n",
    "    to analyze the distribution and relationships of key features in the dataset.\n",
    "    \"\"\"\n",
    "    # Basic statistics summary\n",
    "    print(\"Basic Statistics of Train Data:\")\n",
    "    print(train_df.describe())\n",
    "\n",
    "    # Plot: Distribution of Income\n",
    "    print(\"\\nDistribution of Income:\")\n",
    "    sns.countplot(data=train_df, x='income', palette=\"viridis\")\n",
    "    plt.title('Income Distribution')\n",
    "    plt.xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()  # Ensure labels are not cut off\n",
    "    plt.savefig('eda_income_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Income vs Gender\n",
    "    print(\"\\nIncome vs Gender:\")\n",
    "    sns.countplot(data=train_df, x='sex', hue='income', palette=\"viridis\")\n",
    "    plt.title('Income by Gender')\n",
    "    plt.xlabel('Gender (0 = Female, 1 = Male)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_income_by_gender.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Income vs Education\n",
    "    print(\"\\nIncome vs Education:\")\n",
    " \n",
    "    sns.countplot(data=train_df, y='education', hue='income', palette=\"viridis\",\n",
    "                  order=train_df['education'].value_counts().index)\n",
    "    plt.title('Income by Education Level')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Education Level')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_income_by_education.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Income vs Race\n",
    "    print(\"\\nIncome vs Race:\")\n",
    "    sns.countplot(data=train_df, y='race', hue='income', palette=\"viridis\",\n",
    "                  order=train_df['race'].value_counts().index)\n",
    "    plt.title('Income by Race')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Race')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_income_by_race.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Age Distribution vs Income\n",
    "    print(\"\\nAge Distribution vs Income:\")\n",
    "    sns.histplot(data=train_df, x='age', hue='income', bins=20, kde=True, palette=\"viridis\", multiple=\"stack\")\n",
    "    plt.title('Age Distribution by Income')\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_age_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Marital Status Distribution vs Income\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=train_df, x='marital_stat', hue='income', palette=\"viridis\")\n",
    "    plt.title('Marital Status Distribution by Income')\n",
    "    plt.xlabel('Marital Status')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Income', loc='upper right', labels=['Less than 50K', '50K+'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_marital_stat_income.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot: Boxplots for Wage, Weeks Worked, and Capital Net Gain\n",
    "    print(\"\\nBoxplots for Wage, Weeks Worked, and Capital Net Gain:\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    sns.boxplot(data=train_df, x='income', y='wage_per_hour', ax=axes[0], hue='income', palette=\"viridis\")\n",
    "    axes[0].set_title('Wage per Hour by Income')\n",
    "    axes[0].set_xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    axes[0].set_ylabel('Wage per Hour')\n",
    "\n",
    "    sns.boxplot(data=train_df, x='income', y='weeks_worked_in_year', ax=axes[1], hue='income', palette=\"viridis\")\n",
    "    axes[1].set_title('Weeks Worked by Income')\n",
    "    axes[1].set_xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    axes[1].set_ylabel('Weeks Worked')\n",
    "\n",
    "    sns.boxplot(data=train_df, x='income', y='capital_gains', ax=axes[2], hue='income', palette=\"viridis\")\n",
    "    axes[2].set_title('Capital Gains')\n",
    "    axes[2].set_xlabel('Income (0 = Less than 50K, 1 = 50K+)')\n",
    "    axes[2].set_ylabel('Capital Gains')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_boxplots.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67f628-b55f-4137-8763-eff0a9b7f911",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c1a539-d3de-44dc-b8b0-adf68491e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, splits, test_df):\n",
    "    \"\"\"\n",
    "    Preprocess the data by performing feature engineering, label encoding, \n",
    "    and target variable transformation.\n",
    "    \"\"\"\n",
    "    # Unpack splits\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = splits\n",
    "\n",
    "    # Feature engineering: Calculate net capital gain and drop redundant columns\n",
    "    for dataset in [train_df, X_train_split, X_val_split]:\n",
    "        dataset['capital_net_gain'] = dataset['capital_gains'] - dataset['capital_losses']  # Calculate net capital gain\n",
    "        dataset.drop(columns=['capital_gains', 'capital_losses'], inplace=True)  # Drop original columns\n",
    "\n",
    "    # Apply the same transformation to the test dataset\n",
    "    test_df['capital_net_gain'] = test_df['capital_gains'] - test_df['capital_losses']  # Calculate net capital gain\n",
    "    test_df.drop(columns=['capital_gains', 'capital_losses'], inplace=True)  # Drop original columns\n",
    "\n",
    "    # Initialize a dictionary to store LabelEncoders for each categorical column\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Encode categorical features using LabelEncoder\n",
    "    for column in train_df.select_dtypes(include=['object']).columns:\n",
    "        if column != 'income':  # Skip the target variable\n",
    "            le = LabelEncoder()  # Initialize LabelEncoder\n",
    "            train_df[column] = le.fit_transform(train_df[column])  # Fit and transform training data\n",
    "            X_train_split[column] = le.transform(X_train_split[column])  # Transform training split\n",
    "            X_val_split[column] = le.transform(X_val_split[column])  # Transform validation split\n",
    "            test_df[column] = le.transform(test_df[column])  # Transform test data\n",
    "            label_encoders[column] = le  # Store the encoder\n",
    "\n",
    "    # Convert the target variable (`income`) to binary\n",
    "    train_df['income'] = train_df['income'].apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "    test_df['income'] = test_df['income'].apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "    y_train_split = y_train_split.apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "    y_val_split = y_val_split.apply(lambda x: 1 if x.strip() == '50000+.' else 0)\n",
    "\n",
    "    # Return the preprocessed datasets and the label encoders\n",
    "    return train_df, (X_train_split, X_val_split, y_train_split, y_val_split), test_df, label_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e943526-eed4-486e-a6ec-e88c0c6823cb",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1486d0d8-a37c-4b3e-be9a-9b292ccfb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(splits, test_df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on training, validation, and test datasets.\n",
    "    This function scales the data and retains all features for model training and evaluation.\n",
    "    \"\"\"\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = splits\n",
    "\n",
    "    # Scale data (fit scaler only on training data)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_split_scaled = pd.DataFrame(scaler.fit_transform(X_train_split), columns=X_train_split.columns)\n",
    "    X_val_split_scaled = pd.DataFrame(scaler.transform(X_val_split), columns=X_val_split.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(test_df.drop(columns=['income'])), columns=test_df.drop(columns=['income']).columns)\n",
    "\n",
    "    # Retain all features\n",
    "    selected_features = X_train_split.columns\n",
    "\n",
    "    return (X_train_split_scaled, X_val_split_scaled, y_train_split, y_val_split), X_test_scaled, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2829b2b8-f0cf-42d8-802f-a709a7ed4c96",
   "metadata": {},
   "source": [
    "## Plot Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58ff02ef-038e-4f12-b1f0-2c913923c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, features, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Visualizes the importance of features for models that support the feature_importances_ attribute (e.g., Random Forest, XGBoost).\n",
    "    Saves the feature importance plot to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The machine learning model.\n",
    "        features (list): A list of feature names.\n",
    "        model_name (str): The name of the model (used for saving the plot).\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importances, x='Importance', y='Feature', dodge=False)\n",
    "        plt.title(f\"Feature Importance - {model_name}\")\n",
    "        plt.xlabel(\"Feature Importance Score\")\n",
    "        plt.ylabel(\"Features\")\n",
    "        \n",
    "        # Adjust layout to prevent labels from getting cut off\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        filename = f\"feature_importance_{model_name.replace(' ', '_').lower()}.png\"\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Feature importance plot saved as {filename}\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaa88f-85df-4f38-abc3-fad4930ec5e8",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a07162-1926-4bf1-90d6-eb241ece8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_validation(model, X_train_split, X_val_split, y_train_split, y_val_split):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained model on the validation set.\n",
    "    Outputs a classification report, AUC score, F1-Score, and visualizations.\n",
    "    \"\"\"\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "    predictions = model.predict(X_val_split)\n",
    "    probabilities = model.predict_proba(X_val_split)[:, 1]\n",
    "\n",
    "    # Calculate AUC and F1-Score\n",
    "    auc_score = roc_auc_score(y_val_split, probabilities)\n",
    "    f1 = f1_score(y_val_split, predictions)\n",
    "\n",
    "    print(\"\\nValidation Set Evaluation:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val_split, predictions))\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_val_split, probabilities)\n",
    "    plt.plot(recall, precision, label=f'{type(model).__name__}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (Validation)')\n",
    "    plt.legend()\n",
    "    filename_prc = f\"precision_recall_curve_validation_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_prc)\n",
    "    print(f\"Precision-Recall Curve saved as {filename_prc}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_val_split, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(\"Confusion Matrix (Validation)\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    filename_cm = f\"confusion_matrix_validation_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_cm)\n",
    "    print(f\"Confusion Matrix saved as {filename_cm}\")\n",
    "    plt.close()\n",
    "\n",
    "    return {'AUC': auc_score, 'F1-Score': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3201e-d8e1-47b4-afc9-454141f213b6",
   "metadata": {},
   "source": [
    "## Imbalance & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41560804-ae84-4dd2-8bbc-b74889ef3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_imbalance(X_train_split, y_train_split):\n",
    "    \"\"\"\n",
    "    Balances the training data using SMOTE (Synthetic Minority Oversampling Technique) to handle class imbalance.\n",
    "    \"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train_split, y_train_split)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def train_and_evaluate_models(X_train_split, X_val_split, y_train_split, y_val_split, selected_features):\n",
    "    \"\"\"\n",
    "    Trains and evaluates three machine learning models (Random Forest, Logistic Regression, XGBoost) on the training and validation datasets. \n",
    "    Outputs performance metrics and feature importance plots.\n",
    "    Calls evaluate_on_validation.\n",
    "    Calls plot_feature_importance.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestClassifier(random_state=42, n_estimators=100, class_weight=\"balanced\")\n",
    "    print(\"Random Forest:\")\n",
    "    rf_metrics = evaluate_on_validation(rf_model, X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "    plot_feature_importance(rf_model, selected_features, model_name=\"Random Forest\")\n",
    "    results['Random Forest'] = rf_metrics\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight=\"balanced\")\n",
    "    print(\"Logistic Regression:\")\n",
    "    lr_metrics = evaluate_on_validation(lr_model, X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "    results['Logistic Regression'] = lr_metrics\n",
    "\n",
    "    # XGBoost\n",
    "    scale_pos_weight = (len(y_train_split) - sum(y_train_split)) / sum(y_train_split)\n",
    "    print(f\"Calculated scale_pos_weight for XGBoost: {scale_pos_weight:.2f}\")\n",
    "    xgb_model = XGBClassifier(random_state=42, scale_pos_weight=scale_pos_weight, eval_metric='logloss')\n",
    "    print(\"XGBoost:\")\n",
    "    xgb_metrics = evaluate_on_validation(xgb_model, X_train_split, X_val_split, y_train_split, y_val_split)\n",
    "    plot_feature_importance(xgb_model, selected_features, model_name=\"XGBoost\")\n",
    "    results['XGBoost'] = xgb_metrics\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253acfa-916c-4662-ba95-19b02d547c8c",
   "metadata": {},
   "source": [
    "## Model Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79afd0e0-2bb4-4a8a-b131-60a34fa13855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(results):\n",
    "    \"\"\"\n",
    "    Compares the performance of machine learning models based on their AUC and F1 scores.\n",
    "    Saves comparison plots for both metrics.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing model names as keys and a dictionary of metrics (e.g., AUC, F1-Score) as values.\n",
    "    \"\"\"\n",
    "    print(\"\\n### Model Comparison Recap ###\")\n",
    "    model_names = list(results.keys())\n",
    "    metrics = ['AUC', 'F1-Score']\n",
    "\n",
    "    data = []\n",
    "    for model, metrics_dict in results.items():\n",
    "        row = [model] + [metrics_dict.get(metric, \"N/A\") for metric in metrics]\n",
    "        data.append(row)\n",
    "\n",
    "    comparison_df = pd.DataFrame(data, columns=['Model'] + metrics)\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Plot AUC Comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"AUC\", y=\"Model\", data=comparison_df, palette=\"viridis\")\n",
    "    plt.title(\"Model AUC Comparison\")\n",
    "    plt.xlabel(\"AUC Score\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_auc_comparison.png\", bbox_inches='tight')\n",
    "    print(\"Model AUC Comparison saved as model_auc_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Plot F1-Score Comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"F1-Score\", y=\"Model\", data=comparison_df, palette=\"viridis\")\n",
    "    plt.title(\"Model F1-Score Comparison\")\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_f1_score_comparison.png\", bbox_inches='tight')\n",
    "    print(\"Model F1-Score Comparison saved as model_f1_score_comparison.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364b673-ef67-427b-8768-f70d90ef23bd",
   "metadata": {},
   "source": [
    "## Tune XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dce4b001-566a-4f83-8eaa-4085935699c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train_split, y_train_split, X_val_split, y_val_split):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning on the XGBoost model using RandomizedSearchCV to optimize its parameters for better performance.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "    xgb_random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    xgb_random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "    best_params = xgb_random_search.best_params_\n",
    "    print(\"Best Parameters for XGBoost:\", best_params)\n",
    "\n",
    "    xgb_optimized = XGBClassifier(**best_params, random_state=42, eval_metric='logloss')\n",
    "    xgb_optimized.fit(X_train_split, y_train_split)\n",
    "\n",
    "    return xgb_optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793a35b-d5e7-4cff-a8a9-7c7ff44f1aba",
   "metadata": {},
   "source": [
    "## Evaluate XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4687ae9a-c86a-4213-b8c2-65c2733dedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the final model's performance on the test set.\n",
    "    Outputs a classification report and AUC score.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probabilities)\n",
    "    plt.plot(recall, precision, label=f'{type(model).__name__}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (Test)')\n",
    "    plt.legend()\n",
    "    filename_prc = f\"precision_recall_curve_test_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_prc)\n",
    "    print(f\"Precision-Recall Curve saved as {filename_prc}\")\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix (Test)\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    filename_cm = f\"confusion_matrix_test_{type(model).__name__.lower()}.png\"\n",
    "    plt.savefig(filename_cm)\n",
    "    print(f\"Confusion Matrix saved as {filename_cm}\")\n",
    "    plt.close()\n",
    "\n",
    "    return roc_auc_score(y_test, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdd99a-c26a-48de-a436-30bd0a43b9cd",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27c96f41-ef9b-48f2-a1af-5fc2f68633ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Income distribution (training data) before cleaning:\n",
      "income\n",
      " - 50000.    187141\n",
      " 50000+.      12382\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Income distribution (test data) before cleaning:\n",
      "income\n",
      " - 50000.    93576\n",
      " 50000+.      6186\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training Data: 199523 rows before cleaning, 143468 rows after cleaning.\n",
      "Test Data: 99762 rows before cleaning, 72023 rows after cleaning.\n",
      "\n",
      "Income distribution (training data) after cleaning:\n",
      "income\n",
      " - 50000.    131088\n",
      " 50000+.      12380\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Income distribution (test data) after cleaning:\n",
      "income\n",
      " - 50000.    65837\n",
      " 50000+.      6186\n",
      "Name: count, dtype: int64\n",
      "Basic Statistics of Train Data:\n",
      "                 age  detailed_industry_recode  detailed_occupation_recode  \\\n",
      "count  143468.000000             143468.000000               143468.000000   \n",
      "mean       44.687484                 20.792177                   15.222781   \n",
      "std        17.646275                 18.157176                   14.862288   \n",
      "min        18.000000                  0.000000                    0.000000   \n",
      "25%        31.000000                  0.000000                    0.000000   \n",
      "50%        42.000000                 27.000000                   12.000000   \n",
      "75%        57.000000                 37.000000                   29.000000   \n",
      "max        90.000000                 51.000000                   46.000000   \n",
      "\n",
      "       wage_per_hour  capital_gains  capital_losses  dividends_from_stocks  \\\n",
      "count  143468.000000  143468.000000   143468.000000          143468.000000   \n",
      "mean       75.761557     603.362666       50.793529             273.978978   \n",
      "std       320.851033    5527.632450      316.351165            2335.088207   \n",
      "min         0.000000       0.000000        0.000000               0.000000   \n",
      "25%         0.000000       0.000000        0.000000               0.000000   \n",
      "50%         0.000000       0.000000        0.000000               0.000000   \n",
      "75%         0.000000       0.000000        0.000000               0.000000   \n",
      "max      9999.000000   99999.000000     4608.000000           99999.000000   \n",
      "\n",
      "       num_persons_worked_for_employer  own_business_or_self_employed  \\\n",
      "count                    143468.000000                  143468.000000   \n",
      "mean                          2.649434                       0.236826   \n",
      "std                           2.399311                       0.631626   \n",
      "min                           0.000000                       0.000000   \n",
      "25%                           0.000000                       0.000000   \n",
      "50%                           2.000000                       0.000000   \n",
      "75%                           5.000000                       0.000000   \n",
      "max                           6.000000                       2.000000   \n",
      "\n",
      "       veterans_benefits  weeks_worked_in_year           year  \n",
      "count      143468.000000         143468.000000  143468.000000  \n",
      "mean            1.986192             31.750174      94.500258  \n",
      "std             0.116694             23.465706       0.500002  \n",
      "min             1.000000              0.000000      94.000000  \n",
      "25%             2.000000              0.000000      94.000000  \n",
      "50%             2.000000             50.000000      95.000000  \n",
      "75%             2.000000             52.000000      95.000000  \n",
      "max             2.000000             52.000000      95.000000  \n",
      "\n",
      "Distribution of Income:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/_3kz3q7542g8wy3grh7rl0kc0000gp/T/ipykernel_65244/1736288616.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(data=train_df, x='income', palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Income vs Gender:\n",
      "\n",
      "Income vs Education:\n",
      "\n",
      "Income vs Race:\n",
      "\n",
      "Age Distribution vs Income:\n",
      "\n",
      "Boxplots for Wage, Weeks Worked, and Capital Net Gain:\n",
      "Random Forest:\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     26218\n",
      "           1       0.57      0.55      0.56      2476\n",
      "\n",
      "    accuracy                           0.92     28694\n",
      "   macro avg       0.76      0.76      0.76     28694\n",
      "weighted avg       0.92      0.92      0.92     28694\n",
      "\n",
      "AUC Score: 0.9127\n",
      "F1-Score: 0.5595\n",
      "Precision-Recall Curve saved as precision_recall_curve_validation_randomforestclassifier.png\n",
      "Confusion Matrix saved as confusion_matrix_validation_randomforestclassifier.png\n",
      "Feature importance plot saved as feature_importance_random_forest.png\n",
      "Logistic Regression:\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88     26218\n",
      "           1       0.28      0.82      0.42      2476\n",
      "\n",
      "    accuracy                           0.80     28694\n",
      "   macro avg       0.63      0.81      0.65     28694\n",
      "weighted avg       0.92      0.80      0.84     28694\n",
      "\n",
      "AUC Score: 0.8946\n",
      "F1-Score: 0.4205\n",
      "Precision-Recall Curve saved as precision_recall_curve_validation_logisticregression.png\n",
      "Confusion Matrix saved as confusion_matrix_validation_logisticregression.png\n",
      "Calculated scale_pos_weight for XGBoost: 1.00\n",
      "XGBoost:\n",
      "\n",
      "Validation Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     26218\n",
      "           1       0.61      0.56      0.58      2476\n",
      "\n",
      "    accuracy                           0.93     28694\n",
      "   macro avg       0.78      0.76      0.77     28694\n",
      "weighted avg       0.93      0.93      0.93     28694\n",
      "\n",
      "AUC Score: 0.9238\n",
      "F1-Score: 0.5816\n",
      "Precision-Recall Curve saved as precision_recall_curve_validation_xgbclassifier.png\n",
      "Confusion Matrix saved as confusion_matrix_validation_xgbclassifier.png\n",
      "Feature importance plot saved as feature_importance_xgboost.png\n",
      "\n",
      "### Model Comparison Recap ###\n",
      "                 Model       AUC  F1-Score\n",
      "0        Random Forest  0.912722  0.559526\n",
      "1  Logistic Regression  0.894632  0.420540\n",
      "2              XGBoost  0.923754  0.581626\n",
      "Model AUC Comparison saved as model_auc_comparison.png\n",
      "Model F1-Score Comparison saved as model_f1_score_comparison.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/_3kz3q7542g8wy3grh7rl0kc0000gp/T/ipykernel_65244/2627878770.py:23: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"AUC\", y=\"Model\", data=comparison_df, palette=\"viridis\")\n",
      "/var/folders/ff/_3kz3q7542g8wy3grh7rl0kc0000gp/T/ipykernel_65244/2627878770.py:34: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"F1-Score\", y=\"Model\", data=comparison_df, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2}\n",
      "\n",
      "Test Set Evaluation:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97     65837\n",
      "           1       0.66      0.52      0.58      6186\n",
      "\n",
      "    accuracy                           0.94     72023\n",
      "   macro avg       0.81      0.75      0.77     72023\n",
      "weighted avg       0.93      0.94      0.93     72023\n",
      "\n",
      "Precision-Recall Curve saved as precision_recall_curve_test_xgbclassifier.png\n",
      "Confusion Matrix saved as confusion_matrix_test_xgbclassifier.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_path = '/Users/angieguerrero/Desktop/Dataiku Project/data/census_income_learn.csv'\n",
    "    test_path = '/Users/angieguerrero/Desktop/Dataiku Project/data/census_income_test.csv'\n",
    "\n",
    "    # Load and clean data\n",
    "    train_df, test_df = load_data(train_path, test_path)\n",
    "\n",
    "    # Clean data \n",
    "    train_df, splits, test_df = clean_data(train_df, test_df)\n",
    "\n",
    "    # Perform initial EDA\n",
    "    perform_eda(train_df)\n",
    "\n",
    "    # Preprocessing\n",
    "    train_df, splits, test_df, encoders = preprocess_data(train_df, splits, test_df)\n",
    "\n",
    "    # Feature engineering\n",
    "    splits, X_test_scaled, selected_features = feature_engineering(splits, test_df)\n",
    "\n",
    "    # Handle class imbalance on training data\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = splits\n",
    "    X_train_split, y_train_split = handle_imbalance(X_train_split, y_train_split)\n",
    "\n",
    "    # Train and evaluate models on validation set\n",
    "    results = train_and_evaluate_models(X_train_split, X_val_split, y_train_split, y_val_split, selected_features)\n",
    "\n",
    "    # Compare models\n",
    "    model_comparison(results)\n",
    "\n",
    "    # Hyperparameter tuning for the best model (XGBoost in this case)\n",
    "    xgb_optimized = hyperparameter_tuning(X_train_split, y_train_split, X_val_split, y_val_split)\n",
    "\n",
    "    # Evaluate the optimized model on the test set\n",
    "    y_test = test_df['income']\n",
    "    test_auc = evaluate_on_test(xgb_optimized, X_test_scaled, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
